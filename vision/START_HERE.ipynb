{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J7p406abzgl"
   },
   "source": [
    "# NASSMA 2019: ConvNets and Computer Vision Tutorial\n",
    "\n",
    "## Focus of the tutorial: Study on the efficiency and robustness of convolutional neural networks for image classification\n",
    "\n",
    "### Part I (baseline_start.ipynb): Training a baseline convnet classifier\n",
    "- build a simple classifier given a specific architecture structure\n",
    "- get dataset, apply data augmentation (cropping, flipping)\n",
    "- train classifier\n",
    "- check number of parameters\n",
    "\n",
    "### Part II (visualisation_start.ipynb): Visualise saliency maps (e.g. [paper](https://arxiv.org/pdf/1312.6034v2.pdf))\n",
    "- import an already trained model\n",
    "- visualise the gradients of class probabilities w.r.t inputs to obtain saliency maps\n",
    "- visualise inputs that maximize class probabilities \n",
    "\n",
    "### Part III (distillation_start.ipynb): [Distilling the knowledge](https://arxiv.org/pdf/1503.02531.pdf) from a (larger) teacher model\n",
    "- import an already trained baseline model to use as teacher\n",
    "- use a smaller model as student\n",
    "- add KL distillation loss between teacher and student\n",
    "- train the student classifier with this joint loss\n",
    "\n",
    "### Part IV (optional): Test robusness of models using simple geometric transformations\n",
    "- several recent papers have shown that convnets are not robust to simple geometric transformations, e.g. [paper1](https://arxiv.org/pdf/1805.12177.pdf), [paper2](https://arxiv.org/pdf/1711.09115.pdf)\n",
    "- starting for example from the visualisation colab in part II, write a function that:\n",
    "\n",
    ">- slides crops over Cifar test set images\n",
    ">- feeds the crops into the pre-trained baseline model\n",
    ">- plots predictions per class per crop. Are they stable?\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "- Convolutional models achieve high accuracy in image classification.\n",
    "- Backpropagation is used for training neural networks, but can also be used to visualise what a network has learnt.\n",
    "- Distillation is a simple technique for training smaller models to achieve accuracy similar to larger models. It can be used on its own or in conjunction with separable convolutions. \n",
    "- Although these models are very good, there are still important open research questions about how to make them more robust to various attacks.  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "START_HERE.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
