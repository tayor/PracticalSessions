{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nassma2019/PracticalSessions/blob/master/vision/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocn-alaCjZBc",
        "colab_type": "text"
      },
      "source": [
        "## Part I: Implement and train on Cifar10 dataset a simple baseline for image classification using standard 2D convolutions given the structure below\n",
        "- conv output channels 64, 64, 128, 128, 128, 256, 256, 256, 512, 512, 512\n",
        "- kernel shape (3,3)\n",
        "- strides: 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1\n",
        "- padding: SAME (snt.SAME)\n",
        "- num_output_classes = 10\n",
        "- After each conv layer, add BatchNorm and ReLU.\n",
        "- Use `tf.reduce_mean` to pool spatially the activations at the end -- this way the network can run on inputs of any size.\n",
        "- Project final activations into label space using `snt.Linear`.\n",
        "\n",
        "\n",
        "\n",
        "#### Exercises:\n",
        "\n",
        "1. Fill in the code for the Sonnet module which defines the network, the predictions ops and the loss function ops\n",
        "\n",
        "2. Train the network and see the loss going down. Pay attention to the data augmentation and learning schedule.  \n",
        "\n",
        "3. Understand how BatchNorm works:\n",
        "   \n",
        "\n",
        ">  * remove the update operations and see if the model performs well;\n",
        ">  * keep the update ops, but use `test_local_stats=True`. This will work, but the accuracy will be lower than when using the (training) moving averages.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhWI4Pix5GJw",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na0VvPXmYKp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "# we will use Sonnet on top of TF \n",
        "!pip install -q dm-sonnet\n",
        "import sonnet as snt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Plotting library.\n",
        "from matplotlib import pyplot as plt\n",
        "import pylab as pl\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xlKHOLbhvY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset graph\n",
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g16XweXs2Uq",
        "colab_type": "text"
      },
      "source": [
        "### Download dataset to be used for training and testing\n",
        "- Cifar-10 equivalent of MNIST for natural RGB images\n",
        "- 60000 32x32 colour images in 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\n",
        "- train: 50000; test: 10000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g_EOx07s1XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar10 = tf.keras.datasets.cifar10\n",
        "# (down)load dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt6hU4aQtwpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check sizes of tensors\n",
        "print ('Size of training images')\n",
        "print (train_images.shape)\n",
        "print ('Size of training labels')\n",
        "print (train_labels.shape)\n",
        "print ('Size of test images')\n",
        "print (test_images.shape)\n",
        "print ('Size of test labels')\n",
        "print (test_labels.shape)\n",
        "\n",
        "assert train_images.shape[0] == train_labels.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME1oz3sJkAKX",
        "colab_type": "text"
      },
      "source": [
        "### Display the images\n",
        "\n",
        "The gallery function below shows sample images from the data, together with their labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1xIgRtjvXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_IMAGES = 10\n",
        "def gallery(images, label, title='Input images'):  \n",
        "  class_dict = [u'airplane', u'automobile', u'bird', u'cat', u'deer', u'dog', u'frog', u'horse', u'ship', u'truck']\n",
        "  num_frames, h, w, num_channels = images.shape\n",
        "  num_frames = min(num_frames, MAX_IMAGES)\n",
        "  ff, axes = plt.subplots(1, num_frames,\n",
        "                          figsize=(num_frames, 1),\n",
        "                          subplot_kw={'xticks': [], 'yticks': []})\n",
        "  for i in range(0, num_frames):\n",
        "    if num_channels == 3:\n",
        "      axes[i].imshow(np.squeeze(images[i]))\n",
        "    else:\n",
        "      axes[i].imshow(np.squeeze(images[i]), cmap='gray')\n",
        "    axes[i].set_title(class_dict[label[i][0]])\n",
        "    plt.setp(axes[i].get_xticklabels(), visible=False)\n",
        "    plt.setp(axes[i].get_yticklabels(), visible=False)\n",
        "  ff.subplots_adjust(wspace=0.1)\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN5Kq_xweBhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gallery(train_images, train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHAggitWu94_",
        "colab_type": "text"
      },
      "source": [
        "### Prepare the data for training and testing\n",
        "- for training, we use stochastic optimizers (e.g. SGD, Adam), so we need to sample at random mini-batches from the training dataset\n",
        "- for testing, we iterate sequentially through the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZofMjOuUEOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define dimension of the batches to sample from the datasets\n",
        "BATCH_SIZE_TRAIN = 32 #@param\n",
        "BATCH_SIZE_TEST = 100 #@param\n",
        "\n",
        "# create Dataset objects using the data previously downloaded\n",
        "dataset_train = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "# we shuffle the data and sample repeatedly batches for training\n",
        "batched_dataset_train = dataset_train.shuffle(100000).repeat().batch(BATCH_SIZE_TRAIN)\n",
        "# create iterator to retrieve batches\n",
        "iterator_train = batched_dataset_train.make_one_shot_iterator()\n",
        "# get a training batch of images and labels\n",
        "(batch_train_images, batch_train_labels) = iterator_train.get_next()\n",
        "\n",
        "# check that the shape of the training batches is the expected one\n",
        "print ('Shape of training images')\n",
        "print (batch_train_images)\n",
        "print ('Shape of training labels')\n",
        "print (batch_train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWtdQ0ESxkBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we do the same for test dataset\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "batched_dataset_test = dataset_test.repeat().batch(BATCH_SIZE_TEST)\n",
        "iterator_test = batched_dataset_test.make_one_shot_iterator() \n",
        "(batch_test_images, batch_test_labels) = iterator_test.get_next()\n",
        "print ('Shape of test images')\n",
        "print (batch_test_images)\n",
        "print ('Shape of test labels')\n",
        "print (batch_test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4iKpKNB-c74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Squeeze labels and convert from uint8 to int32 - required below by the loss op\n",
        "batch_test_labels = tf.cast(tf.squeeze(batch_test_labels), tf.int32)\n",
        "batch_train_labels = tf.cast(tf.squeeze(batch_train_labels), tf.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5rWCfPp-50y",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess input for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PS2GjTxRZx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation used for train preprocessing\n",
        "# - scale image to [-1 , 1]\n",
        "# - get a random crop\n",
        "# - apply horizontal flip randomly\n",
        "\n",
        "def train_image_preprocess(h, w, random_flip=True):\n",
        "  \"\"\"Image processing required for training the model.\"\"\"\n",
        "  \n",
        "  def random_flip_left_right(image, flip_index, seed=None):\n",
        "    shape = image.get_shape()\n",
        "    if shape.ndims == 3 or shape.ndims is None:\n",
        "      uniform_random = tf.random_uniform([], 0, 1.0, seed=seed)\n",
        "      mirror_cond = tf.less(uniform_random, .5)\n",
        "      result = tf.cond(\n",
        "          mirror_cond,\n",
        "          lambda: tf.reverse(image, [flip_index]),\n",
        "          lambda: image\n",
        "      )\n",
        "      return result\n",
        "    elif shape.ndims == 4:\n",
        "      uniform_random = tf.random_uniform(\n",
        "          [tf.shape(image)[0]], 0, 1.0, seed=seed\n",
        "      )\n",
        "      mirror_cond = tf.less(uniform_random, .5)\n",
        "      return tf.where(\n",
        "          mirror_cond,\n",
        "          image,\n",
        "          tf.map_fn(lambda x: tf.reverse(x, [flip_index]), image, dtype=image.dtype)\n",
        "      )\n",
        "    else:\n",
        "      raise ValueError(\"\\'image\\' must have either 3 or 4 dimensions.\")\n",
        "\n",
        "  def fn(image):\n",
        "    # Ensure the data is in range [-1, 1].\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    image = image * 2.0 - 1.0\n",
        "    # Randomly choose a (h, w, 3) patch.\n",
        "    image = tf.random_crop(image, size=(BATCH_SIZE_TRAIN, h, w, 3))\n",
        "    # Randomly flip the image.\n",
        "    image = random_flip_left_right(image, 2)\n",
        "    return image\n",
        "\n",
        "  return fn\n",
        "\n",
        "# Test preprocessing: only scale to [-1,1].\n",
        "def test_image_preprocess():\n",
        "  def fn(image):\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    image = image * 2.0 - 1.0\n",
        "    return image\n",
        "  return fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTsLFmVCF585",
        "colab_type": "text"
      },
      "source": [
        "### Define the network \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnGir_ARmVSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Exercise.\n",
        "class Baseline(snt.AbstractModule):\n",
        "  \n",
        "  def __init__(self, num_classes, name=\"baseline\"):\n",
        "    super(Baseline, self).__init__(name=name)\n",
        "    self._num_classes = num_classes\n",
        "    self._output_channels = [\n",
        "        64, 64, 128, 128, 128, 256, 256, 256, 512, 512, 512\n",
        "        ]\n",
        "    #############\n",
        "    #           #\n",
        "    # YOUR CODE #\n",
        "    #           #\n",
        "    #############\n",
        "   \n",
        "  def _build(self, inputs, is_training=None, test_local_stats=False):\n",
        "    \n",
        "    #############\n",
        "    #           #\n",
        "    # YOUR CODE #\n",
        "    #           #\n",
        "    #############\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2scBoc09ZsO4",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution.\n",
        "class Baseline(snt.AbstractModule):\n",
        "  \n",
        "  def __init__(self, num_classes, name=\"baseline\"):\n",
        "    super(Baseline, self).__init__(name=name)\n",
        "    self._num_classes = num_classes\n",
        "    self._output_channels = [\n",
        "        64, 64, 128, 128, 128, 256, 256, 256, 512, 512, 512\n",
        "        ]\n",
        "    self._num_layers = len(self._output_channels)\n",
        "    self._kernel_shapes = [[3, 3]] * self._num_layers  # All kernels are 3x3.\n",
        "    self._strides = [1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1]\n",
        "    self._paddings = [snt.SAME] * self._num_layers\n",
        "   \n",
        "  def _build(self, inputs, is_training=None, test_local_stats=False):\n",
        "    net = inputs\n",
        "    # instantiate all the convolutional layers \n",
        "    layers = [snt.Conv2D(name=\"conv_2d_{}\".format(i),\n",
        "                         output_channels=self._output_channels[i],\n",
        "                         kernel_shape=self._kernel_shapes[i],\n",
        "                         stride=self._strides[i],\n",
        "                         padding=self._paddings[i],\n",
        "                         use_bias=True) for i in xrange(self._num_layers)]\n",
        "    # connect them to the graph, adding batch norm and relu non-linearity\n",
        "    for i, layer in enumerate(layers):\n",
        "      net = layer(net)\n",
        "      bn = snt.BatchNorm(name=\"batch_norm_{}\".format(i))\n",
        "      net = bn(net, is_training=is_training, test_local_stats=test_local_stats)\n",
        "      net = tf.nn.relu(net)\n",
        "\n",
        "    net = tf.reduce_mean(net, reduction_indices=[1, 2], keepdims=False,\n",
        "                         name=\"avg_pool\")\n",
        "\n",
        "    logits = snt.Linear(self._num_classes)(net)\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD8IR90m_0-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get number of parameters in a scope by iterating through the trainable variables\n",
        "def get_num_params(scope):\n",
        "  total_parameters = 0\n",
        "  for variable in tf.trainable_variables(scope):\n",
        "    # shape is an array of tf.Dimension\n",
        "    shape = variable.get_shape()\n",
        "    variable_parameters = 1\n",
        "    for dim in shape:\n",
        "      variable_parameters *= dim.value\n",
        "    total_parameters += variable_parameters\n",
        "  return total_parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inX9OlHW5xWR",
        "colab_type": "text"
      },
      "source": [
        "### Instantiate the model and connect to data \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZzlpO0oJFZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First define the preprocessing ops for the train/test data\n",
        "crop_height = 24 #@param\n",
        "cropt_width = 24 #@param\n",
        "preprocess_fn_train = train_image_preprocess(crop_height, cropt_width)\n",
        "preprocess_fn_test = test_image_preprocess()\n",
        "\n",
        "num_classes = 10 #@param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZlB8Ao5CZJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate the model\n",
        "with tf.variable_scope(\"baseline\"):\n",
        "  baseline_model = Baseline(num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmMKoVblmecR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Exercise.\n",
        "# Get predictions from the model; use the corresponding preprocess ops and is_training flag\n",
        "# train_predictions = ############## YOUR CODE ##############\n",
        "# print(train_predictions)\n",
        "\n",
        "# test_predictions = ############## YOUR CODE ##############\n",
        "# print(test_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt87AMfB6VKt",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution.\n",
        "# Get predictions from the model; use the corresponding preprocess ops and is_training flag\n",
        "train_predictions = baseline_model(preprocess_fn_train(batch_train_images), is_training=True)\n",
        "print(train_predictions)\n",
        "\n",
        "test_predictions = baseline_model(preprocess_fn_test(batch_test_images), is_training=False)\n",
        "print(test_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JUYuIR3Bv8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get number of parameters in the model. Can you obtain this number by hand?\n",
        "print (\"Total number of parameters of baseline model\")\n",
        "print (get_num_params(\"baseline\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpxLmb3sJa0o",
        "colab_type": "text"
      },
      "source": [
        "### Define the loss to be minimized during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9juCwywKqe5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss(logits=None, labels=None):\n",
        "  # We reduce over batch dimension, to ensure the loss is a scalar.   \n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          labels=labels, logits=logits))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZweG17cmwLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Exercise.\n",
        "# Define train and test loss ops\n",
        "# train_loss = ############## YOUR CODE ##############\n",
        "# test_loss = ############## YOUR CODE ##############"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQBnrZPb7XcI",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution.\n",
        "# Define train and test loss functions\n",
        "train_loss = get_loss(labels=batch_train_labels, logits=train_predictions)\n",
        "test_loss = get_loss(labels=batch_test_labels, logits=test_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K4VMXej8Fem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For evaluation, we look at top_k_accuracy since it's easier to interpret; normally k=1 or k=5\n",
        "def top_k_accuracy(k, labels, logits):\n",
        "  in_top_k = tf.nn.in_top_k(predictions=tf.squeeze(logits), targets=labels, k=k)\n",
        "  return tf.reduce_mean(tf.cast(in_top_k, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGyLJwJ408ZZ",
        "colab_type": "text"
      },
      "source": [
        "### Create the optimizer\n",
        "\n",
        "We will use the Momentum optimizer, but other optimizers such as Adam or AdaGrad can be used. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8V7fy_U2yY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimizer(step):\n",
        "  \"\"\"Get the optimizer used for training.\"\"\"\n",
        "  lr_init = 0.1 # initial value for the learning rate\n",
        "  lr_schedule = (40e3, 60e3, 80e3) # after how many iterations to reduce the learning rate\n",
        "  lr_schedule = tf.to_int64(lr_schedule)\n",
        "  lr_factor = 0.1 # reduce learning rate by this factor\n",
        "  \n",
        "  \n",
        "  num_epochs = tf.reduce_sum(tf.to_float(step >= lr_schedule))\n",
        "  lr = lr_init * lr_factor**num_epochs\n",
        "\n",
        "  return tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTFLYiWv8Z_n",
        "colab_type": "text"
      },
      "source": [
        "### Set up the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5tjjaENpgmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a global step that is incremented during training; useful for e.g. learning rate annealing\n",
        "global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "# instantiate the optimizer\n",
        "optimizer = get_optimizer(global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvw-WbFDeTl0",
        "colab_type": "text"
      },
      "source": [
        "### BatchNorm ops (implementation detail, OK to skim through this section)\n",
        "\n",
        "Batch normalization requires updating the moving averages during training, so they can be used during testing instead of the statistics of the test batch. However, there is no direct dependency between the moving averages and the train ops. Hence running only the train ops will not update the moving averages. \n",
        "\n",
        "The Sonnet BatchNorm module ensures that the moving average updates are added to the global UPDATE_OPS collections. So all we need to do is to group the train ops with the update ops.\n",
        "\n",
        "To find out more about collections: https://www.tensorflow.org/api_guides/python/framework#Graph_collections\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTAm5Nfsc2Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get training ops\n",
        "training_baseline_op = optimizer.minimize(train_loss, global_step)\n",
        "\n",
        "# Retrieve the update ops, which contain the moving average ops\n",
        "update_ops = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS))\n",
        "\n",
        "# Manually add the update ops to the dependency path executed at each training iteration\n",
        "training_baseline_op = tf.group(training_baseline_op, update_ops)\n",
        "\n",
        "# For exercise 3, comment the line above and see if the model performs well."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aGft4Zhppdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get test ops\n",
        "test_acc_baseline_op = top_k_accuracy(1, batch_test_labels, test_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF0oEXrHFB7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that takes a list of losses and plots them.\n",
        "def plot_losses(loss_list, steps):\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(pl.gcf())\n",
        "  pl.plot(steps, loss_list, c='b')\n",
        "  time.sleep(1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2unavvlpBFn",
        "colab_type": "text"
      },
      "source": [
        "### Training params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-m8-e5vpUIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define number of training iterations and reporting intervals\n",
        "TRAIN_ITERS = 90e3 #@param\n",
        "REPORT_TRAIN_EVERY = 10 #@param\n",
        "PLOT_EVERY = 500 #@param\n",
        "REPORT_TEST_EVERY = 1000 #@param\n",
        "TEST_ITERS = 10 #@param\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2h2tdVrxLTR",
        "colab_type": "text"
      },
      "source": [
        "### Train the model (you can stop the training once you observe the loss going down and the test accuracy going up). Running the full training gives around 94% accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elUiAs1S1gkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the session and initialize variables\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# Question: What is the accuracy of the model at iteration 0, i.e. before training starts? \n",
        "train_iter = 0\n",
        "losses = []\n",
        "steps = []\n",
        "for train_iter in range(int(TRAIN_ITERS)):\n",
        "  _, train_loss_np = sess.run([training_baseline_op, train_loss])\n",
        "  \n",
        "  if (train_iter % REPORT_TRAIN_EVERY) == 0:\n",
        "    losses.append(train_loss_np)\n",
        "    steps.append(train_iter)\n",
        "  if (train_iter % PLOT_EVERY) == 0:\n",
        "    plot_losses(losses, steps)    \n",
        "    \n",
        "  if (train_iter % REPORT_TEST_EVERY) == 0:\n",
        "    avg_acc = 0.0\n",
        "    for test_iter in range(TEST_ITERS):\n",
        "      acc = sess.run(test_acc_baseline_op)\n",
        "      avg_acc += acc\n",
        "      \n",
        "    avg_acc /= (TEST_ITERS)\n",
        "    print ('Test acc at iter {0:5d} out of {1:5d} is {2:.2f}%'.format(int(train_iter), int(TRAIN_ITERS), avg_acc*100.0))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}