{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comprehensive Exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nassma2019/PracticalSessions/blob/master/introductory/Comprehensive_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iYx6oEVKcE_L"
      },
      "source": [
        "# Exercise: putting everything together\n",
        "\n",
        "\n",
        "In this you will write code for a model that learns to classify mnist digits. You will use sonnet and tensorflow, tracking training progress with matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TGBJLkR_cI3L",
        "colab": {}
      },
      "source": [
        "# Install dm-sonnet with pip + some useful imports.\n",
        "!pip install dm-sonnet\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import random\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sonnet as snt\n",
        "import datetime\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.stats import multivariate_normal\n",
        "from IPython.display import clear_output, Image, display, HTML\n",
        "\n",
        "sns.set_style('ticks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5gkBQpjJlCgP",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nO_tMPdncmVy",
        "colab": {}
      },
      "source": [
        "# Fetch the mnist data from tf.keras.datasets.mnist.\n",
        "\n",
        "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Check what the data is like:\n",
        "print('Training dataset:')\n",
        "train_input, train_label = mnist_train\n",
        "print('* input shape:', train_input.shape)\n",
        "print('* input min, mean, max:', train_input.min(), train_input.mean(), train_input.max())\n",
        "print('* input dtype:', train_input.dtype)\n",
        "print('* label shape:', train_label.shape)\n",
        "print('* label min, mean, max:', train_label.min(), train_label.mean(), train_label.max())\n",
        "print('* label dtype:', train_label.dtype)\n",
        "\n",
        "test_input, test_label = mnist_test\n",
        "print('Number of test examples:', test_input.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "utL4ZmLrepoH"
      },
      "source": [
        "Normalize the data into the \\[0, 1\\] interval. It's also a good idea to check the class distribution, but here we know that this is OK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60_4wXEPe7Ig",
        "colab": {}
      },
      "source": [
        "# Normalize both train_input and test_input so that it is in [0, 1].\n",
        "#\n",
        "# Also ensure the following data types:\n",
        "#\n",
        "# * train_input and test_input need to be np.float32.\n",
        "# * the labels need to be converted to np.int32.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGLwcWA8FcZE",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution\n",
        "train_input = train_input.astype(np.float32) / 255.\n",
        "test_input = test_input.astype(np.float32) / 255.\n",
        "\n",
        "train_label = train_label.astype(np.int32)\n",
        "test_label = test_label.astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JDwRkDiYfzVO",
        "colab": {}
      },
      "source": [
        "#@title Defining the `gallery` function.\n",
        "# We can visualize the first few training examples using matplotlib.imshow()\n",
        "# in combination with the gallery function below (define in the numpy/plotting\n",
        "# notebook):\n",
        "\n",
        "def gallery(array, ncols=10, rescale=True):\n",
        "  \"\"\"Take a batch of images and arrange them in a grid.\n",
        "  \n",
        "  Args:\n",
        "    array: of shape batch_size x height x width x 3. The final x3 in the shape\n",
        "      is optional.\n",
        "    ncols: number of columns to have in the grid\n",
        "    rescale: if true (default), increases the intensity of the images.\n",
        "    \n",
        "  Returns:\n",
        "    A numpy array which contains the batch of images arranged into\n",
        "    a grid.\n",
        "  \"\"\"\n",
        "  if rescale:\n",
        "    array = (array + 1.) / 2\n",
        "  nindex, height, width = array.shape[:3]\n",
        "\n",
        "  nrows = nindex//ncols\n",
        "  assert nindex == nrows*ncols\n",
        "\n",
        "  # want result.shape = (height*nrows, width*ncols, intensity?)\n",
        "  abstract_grid_shape = [nrows, ncols, height, width]\n",
        "  image_grid_shape = [height*nrows, width*ncols]\n",
        "  if len(array.shape) == 4:\n",
        "    intensity = array.shape[3]\n",
        "    abstract_grid_shape.append(intensity)\n",
        "    image_grid_shape.append(intensity)\n",
        " \n",
        "  result = (array.reshape(*abstract_grid_shape)\n",
        "            .swapaxes(1,2)\n",
        "            .reshape(*image_grid_shape))\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1WQD1huVgV8Y",
        "colab": {}
      },
      "source": [
        "# Show the first 6 training images on a 1x6 grid.\n",
        "# Remember to use grayscale plotting.\n",
        "# Also print their corresponding labels in the same order.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yBVTwcPFcZO",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution.\n",
        "\n",
        "plt.figure()\n",
        "plt.gray()\n",
        "plt.axis('off')\n",
        "plt.imshow(gallery(train_input[:6], ncols=6, rescale=False))\n",
        "plt.show()\n",
        "\n",
        "print(train_label[:6])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6VZdwYo_fUpo",
        "colab": {}
      },
      "source": [
        "# Write a function that turns the data into tensorflow datasets and into\n",
        "# tensors corresponding to batches of examples, returning these tensors.\n",
        "#\n",
        "# The train data should be\n",
        "#\n",
        "# * shuffled across the full dataset\n",
        "# * repeated indefinitely\n",
        "# * batched at size 64.\n",
        "#\n",
        "# Simply batch the test data.\n",
        "#\n",
        "# IMPORTANT: Add a final (singleton) axis to the inputs; the conv nets that\n",
        "# we will use will expect this.\n",
        "#\n",
        "# This function, `get_tf_data()`, will be used in one of the cells below.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4FPEreEFcZT",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution.\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "\n",
        "def _get_data_tensor(dataset):\n",
        "  return dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "\n",
        "def get_tf_data():\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "      np.expand_dims(train_input, axis=-1), train_label))\n",
        "  train_dataset = train_dataset.shuffle(60000).repeat().batch(BATCH_SIZE)\n",
        "  train_data = _get_data_tensor(train_dataset)\n",
        "\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "      np.expand_dims(test_input, axis=-1), test_label))\n",
        "  test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "  test_data = _get_data_tensor(test_dataset)\n",
        "  \n",
        "  return train_data, test_data\n",
        "\n",
        "\n",
        "# Check the function outputs.\n",
        "get_tf_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3JcANwNfHuQ",
        "colab": {}
      },
      "source": [
        "# Make a sonnet module that has the following structure:\n",
        "#\n",
        "# 1. sonnet Conv2D with 16 channes, kernel shape 3, stride 1, padding 'SAME'\n",
        "# 2. max pooling with window_shape [3, 3], srides [2, 2], padding 'SAME'\n",
        "# 3. ReLU\n",
        "# 4. sonnet Conv2D with 16 channes, kernel shape 3, stride 1, padding 'SAME'\n",
        "# 5. Flatten the final conv features using snt.BatchFlatten\n",
        "# 5. A (dense) Linear layer with output_size = 10, the number of classes.\n",
        "#\n",
        "# You can write the sonnet module yourself, or use the helper module\n",
        "# snt.Sequential([..layers..to..connect..]).\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9mWia81FcZZ",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution.\n",
        "def make_network():\n",
        "  \n",
        "  def make_conv_layer():\n",
        "    return snt.Conv2D(16, 3, stride=1, padding='SAME')\n",
        "  \n",
        "  def pooling_layer(inputs):\n",
        "    return tf.nn.pool(\n",
        "        inputs,\n",
        "        window_shape=[3, 3],\n",
        "        pooling_type='MAX',\n",
        "        padding='SAME',\n",
        "        strides=[2, 2])\n",
        "\n",
        "  return snt.Sequential([\n",
        "      make_conv_layer(),\n",
        "      pooling_layer,\n",
        "      tf.nn.relu,\n",
        "      make_conv_layer(),\n",
        "      snt.BatchFlatten(),\n",
        "      snt.Linear(10),\n",
        "  ])\n",
        "\n",
        "\n",
        "# Check the function outputs.\n",
        "make_network()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YRp2hrGofH7f",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "(train_inputs, train_labels), (test_inputs, test_labels) = get_tf_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g7daVkyoqS9p",
        "colab": {}
      },
      "source": [
        "# 1. Instantiate a model\n",
        "# 2. Hook it up to the training data,\n",
        "# 3. Use the `tf.nn.sparse_softmax_cross_entropy_with_logits` op to define the loss\n",
        "# 4. Define the train_op that minimizes the loss (averaged over the batch)\n",
        "#    using the `GradientDescentOptimizer`. Set the learning rate to 0.01.\n",
        "# 5. Get the initialization op."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A_RwmAUFcZr",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution.\n",
        "model = make_network()\n",
        "\n",
        "train_outputs = model(train_inputs)\n",
        "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "    labels=train_labels,\n",
        "    logits=train_outputs,\n",
        ")\n",
        "loss = tf.reduce_mean(loss)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "init_op = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wvmlucn6vbSD",
        "colab": {}
      },
      "source": [
        "# Write a function that takes a list of losses and plots them."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4CBgN_kFcZv",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution.\n",
        "def plot_losses(loss_list):\n",
        "  plt.figure()\n",
        "  plt.title('Losses')\n",
        "  plt.plot(loss_list, c='b')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tufk2Xa2qTEI",
        "colab": {}
      },
      "source": [
        "# Run the training loop, keeping track of losses and potentially the accuracy\n",
        "# on the training set. Plot the loss curve intermittently.\n",
        "#\n",
        "# The simplest solution would add a new plot with each plotting call. You\n",
        "# can play with the frequency of plotting (and recording) a bit in order\n",
        "# to find something that works.\n",
        "#\n",
        "# Based on the loss curves, decide how to set your total number of training\n",
        "# iterations. Once you are satified, add some code that evaluates your\n",
        "# prediction accuracy (not loss!) on the test set.\n",
        "#\n",
        "# Note that the outputs from the network are logits; for prediction accuracy\n",
        "# we can pick the most likely label and see if it is correct.\n",
        "\n",
        "# The accuracy you should expect:\n",
        "#\n",
        "# * Roughly 90% after 1000 training steps.\n",
        "# * 97-98% after 10k training steps.\n",
        "#\n",
        "# First iterate with 1k steps, if that works, train for 10k. 10k steps will\n",
        "# be roughly 6 minutes on CPU.\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0fwSrI-c2Cn3",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Solution.\n",
        "TRAIN_ITERS = int(10000)\n",
        "RECORD_PERIOD = 10\n",
        "PLOT_PERIOD = int(500)\n",
        "\n",
        "\n",
        "def get_accuracy(predictions, true_labels):\n",
        "  assert predictions.shape == true_labels.shape  \n",
        "  num_elements = predictions.shape[0]\n",
        "  num_correct = np.count_nonzero(np.isclose(predictions, true_labels))\n",
        "  return num_correct / num_elements\n",
        "  \n",
        "\n",
        "def get_predictions_and_true_labels(\n",
        "    session, outputs_tensor, labels_tensor, num_batches=-1):\n",
        "  all_predictions = []\n",
        "  all_true_labels = []\n",
        "  while num_batches != 0:\n",
        "    try:\n",
        "      outputs_np, labels_np = session.run([outputs_tensor, labels_tensor])\n",
        "      if num_batches > 0:\n",
        "        num_batches -= 1\n",
        "\n",
        "      assert len(outputs_np.shape) == 2\n",
        "      predictions = outputs_np.argmax(axis=1)\n",
        "      all_predictions.append(predictions)\n",
        "      all_true_labels.append(labels_np)\n",
        "    except tf.errors.OutOfRangeError:\n",
        "      break\n",
        "      \n",
        "  # Concatenate all collected data.\n",
        "  return (\n",
        "      np.concatenate(all_predictions),\n",
        "      np.concatenate(all_true_labels),      \n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "start_time = datetime.datetime.now()\n",
        "losses = []\n",
        "with tf.Session() as session:\n",
        "  session.run(init_op)\n",
        "  \n",
        "  for train_iter in range(TRAIN_ITERS):\n",
        "    _, loss_np = session.run([train_op, loss])\n",
        "    \n",
        "    if (train_iter % RECORD_PERIOD) == 0:\n",
        "      losses.append(loss_np)\n",
        "  \n",
        "    if (train_iter % PLOT_PERIOD) == 0:\n",
        "      print('train iter {}, elapsed {}'.format(\n",
        "          train_iter, datetime.datetime.now() - start_time))\n",
        "      plot_losses(losses)\n",
        "      predictions, true_labels = get_predictions_and_true_labels(\n",
        "          session, train_outputs, train_labels, num_batches=30)\n",
        "      print('Accuracy on training set:', get_accuracy(predictions, true_labels))\n",
        "\n",
        "  print('FINAL pred accuracy after {} steps and {} time:'.format(\n",
        "      train_iter+1, datetime.datetime.now() - start_time))\n",
        "  print('* Train:', get_accuracy(\n",
        "      *get_predictions_and_true_labels(\n",
        "          session, train_outputs, train_labels, num_batches=30)\n",
        "  ))\n",
        "  print('* Test:', get_accuracy(\n",
        "      *get_predictions_and_true_labels(\n",
        "          session, model(test_inputs), test_labels, num_batches=-1)\n",
        "  ))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}